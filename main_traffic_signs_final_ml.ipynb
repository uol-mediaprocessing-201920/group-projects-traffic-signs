{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "main_traffic-signs.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9kQcKta5U9XB",
        "VhuhtLmlTD4D",
        "z0uE5MM-46XU",
        "pKwyVJGemcpx",
        "h9JwskgFWj4e",
        "u7MaCimzT8HM",
        "VjlLd7NYWj5G",
        "gzKnx4bzW2tQ",
        "e6czrwtbT8Ho",
        "NAwr4eGVZuum"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9kQcKta5U9XB"
      },
      "source": [
        "#  Assisting children while biking\n",
        "\n",
        "### Gruppe 7: Maximilian HÃ¶rnis, Florian Schwarm, Dennis Rupprecht"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA3gWpqGQrSP",
        "colab_type": "text"
      },
      "source": [
        "# Table of Content\n",
        "## 1. Introduction\n",
        "## 2. Imports\n",
        "## 3. Color recognition\n",
        "## 4. Classification by Contours\n",
        "## 5. Machine Learning\n",
        "## 6. Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhuhtLmlTD4D",
        "colab_type": "text"
      },
      "source": [
        "# 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uGNSSQYTAgs",
        "colab_type": "text"
      },
      "source": [
        "## Motivation \n",
        "- Children are active participants in road traffic\n",
        "- Traffic signs might not be obvious to them\n",
        "- Handheld technology device (smartphones etc.) usage is prevalent among children "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BPiTRiyMUyjp"
      },
      "source": [
        "## Idea\n",
        "\n",
        "Recognize and classify traffic signs automatically and display them in\n",
        "an easy to understand assistive technology for children, without distracting them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0MIfdy9bUyjs"
      },
      "source": [
        "## Tasks\n",
        "    \n",
        "- Recognizing and classifiying traffic signs\n",
        "- Interpretation of images in a way that is\n",
        "    - a) sufficiently comprehensible for children and\n",
        "    - b) not distracting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "12OZ1BgyUyju"
      },
      "source": [
        "## Possible Extensions\n",
        "\n",
        "- Detection of intersections without relying on traffic signs\n",
        "- Detection of traffic lights and their current position (red, green, yellow)\n",
        "- Applying the traffic sign classifier to live video (or recordings)\n",
        "- Integrating the system into a mobile application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VSqMLdJ7Uyjx"
      },
      "source": [
        "## Possible Issues\n",
        "\n",
        "- Recognizing dirty or occluded signs\n",
        "\n",
        "- Determining which street a sign refers to and how long it is valid\n",
        "- Reliable detection of intersections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0uE5MM-46XU",
        "colab_type": "text"
      },
      "source": [
        "# 2. Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtEKBaAz4zLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install Augmentor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ddr7z8iW4-bs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os.path\n",
        "import Augmentor\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKwyVJGemcpx",
        "colab_type": "text"
      },
      "source": [
        "# 3. Color recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dP9RZpMY354b",
        "colab_type": "text"
      },
      "source": [
        "First we created a method to display a list of images with matplotlib. This will be used from time to time later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_kxJ1AJwaCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to show all images\n",
        "def show_images(images, cols = 1, titles = None):\n",
        "    \"\"\"Display a list of images in a single figure with matplotlib.\n",
        "    \n",
        "    Parameters\n",
        "    ---------\n",
        "    images: List of np.arrays compatible with plt.imshow.\n",
        "    \n",
        "    cols (Default = 1): Number of columns in figure (number of rows is \n",
        "                        set to np.ceil(n_images/float(cols))).\n",
        "    \n",
        "    titles: List of titles corresponding to each image. Must have\n",
        "            the same length as titles.\n",
        "    \"\"\"\n",
        "    assert((titles is None)or (len(images) == len(titles)))\n",
        "    n_images = len(images)\n",
        "    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n",
        "    fig = plt.figure()\n",
        "    for n, (image, title) in enumerate(zip(images, titles)):\n",
        "        a = fig.add_subplot(cols, np.ceil(n_images/float(cols)), n + 1)\n",
        "        if image.ndim == 2:\n",
        "            plt.gray()\n",
        "        plt.imshow(image)\n",
        "        plt.axis(\"off\")\n",
        "        a.set_title(title)\n",
        "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBR6Kw3H4RS2",
        "colab_type": "text"
      },
      "source": [
        "At the beginning we converted the images into the HSV color space. Later we chose the HLS color space, because there the color boundaries can be defined less sensitive to illumination. In both methods, the lower and upper color limits are defined and a mask that filters out the colors red, blue and yellow is created. The HSV method only filters out red. Afterwards edges are made visible with a Canny Filter and optimized by a closing. A list with all images from the individual steps is returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48h7T9WkuMN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def image_transformation_hsv(image_name):\n",
        "  image1 = cv.imread(image_name)\n",
        "  image1 = cv.cvtColor(image1, cv.COLOR_BGR2RGB)\n",
        "  img1_hsv = cv.cvtColor(image1, cv.COLOR_RGB2HSV)\n",
        "\n",
        "  # Define the colors.\n",
        "  lower_color = np.array([100,100,100])\n",
        "  upper_color = np.array([179,255,255])\n",
        "\n",
        "  mask = cv.inRange(img1_hsv, lower_color, upper_color)\n",
        "\n",
        "  edged = cv.Canny(mask, 10, 250)\n",
        "  #applying closing function \n",
        "  kernel = cv.getStructuringElement(cv.MORPH_RECT, (7, 7))\n",
        "  closed = cv.morphologyEx(edged, cv.MORPH_CLOSE, kernel)\n",
        "\n",
        "  images = [image1, img1_hsv, mask, edged, kernel, closed]\n",
        "  titles = [\"image\",\"img_hsv\", \"mask\", \"edged\", \"kernel\", \"closed\"]\n",
        "  # show_images(images,1,titles)\n",
        "  return images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kV2eAhWmdsM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def image_transformation_hls(image_name):\n",
        "  image2 = cv.imread(image_name)\n",
        "  image2 = cv.cvtColor(image2, cv.COLOR_BGR2RGB)\n",
        "  img2_hls = cv.cvtColor(image2, cv.COLOR_RGB2HLS)\n",
        "  maskes = []\n",
        "  mask = np.zeros((img2_hls.shape[0], img2_hls.shape[1], 1), dtype = \"uint8\")\n",
        "  # Define the colors.\n",
        "  hls_range = [[np.array([172,40,50]), np.array([180,130,130])],\n",
        "               [np.array([0,40,50]), np.array([5,130,130])],\n",
        "               [np.array([145,90,90]), np.array([155,130,130])]]\n",
        "\n",
        "  for hls_ran in hls_range:\n",
        "    maskes.append(cv.inRange(img2_hls, hls_ran[0], hls_ran[1]))\n",
        "  \n",
        "  for mask_solo in maskes:\n",
        "    mask = cv.bitwise_or(mask, mask_solo)\n",
        "\n",
        "  edged = cv.Canny(mask, 10, 250)\n",
        "  #applying closing function \n",
        "  kernel = cv.getStructuringElement(cv.MORPH_RECT, (7, 7))\n",
        "  closed = cv.morphologyEx(edged, cv.MORPH_CLOSE, kernel)\n",
        "\n",
        "  images = [image2, img2_hls, mask, edged, kernel, closed]\n",
        "  titles = [\"image\",\"img2_hls\", \"mask\", \"edged\", \"kernel\", \"closed\"]\n",
        "  # show_images(images,1,titles)\n",
        "  return images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e3u7r9hWWXa",
        "colab_type": "text"
      },
      "source": [
        "# 4. Classification by Contours"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "h9JwskgFWj4e"
      },
      "source": [
        "# 5. Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyYxgiEpWqnF",
        "colab_type": "text"
      },
      "source": [
        "## The dataset\n",
        "\n",
        "The dataset we are using is a modified version of the German Traffic Sign Recognition Benchmark (GTSRB) dataset [1]. It consists of images of 43 types of traffic signs and their corresponding labels.\n",
        "\n",
        "[1] http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWzVO5ddVs4Z",
        "colab_type": "text"
      },
      "source": [
        "Since the data augmentation that we perform on the dataset later on takes a considerable amount of time (since it is executed on the CPU only), we have prepared a dataset consisting of the GTSRB training data set reduced to the classes of traffic signs we want to use and augmented to have 2000 images for each traffic sign class. If you want to use this modified dataset, execute the cell below as is, otherwise set the variable to `False` in order to do the dataset preparation from scratch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOoXhlbFWNTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "skip_preparation = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b41zTmjoXMFk",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "if skip_preparation:\n",
        "  !wget -c https://www.dropbox.com/s/dl/56pxs90xcb8z4mc/training_data_final.zip -O training_data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5pvKUd7jT8Gr",
        "colab": {}
      },
      "source": [
        "if not skip_preparation:\n",
        "  !wget -c https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Training_Images.zip -O training_data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A9ej1_GWVcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r GTSRB\n",
        "!rm -r training_data\n",
        "\n",
        "with zipfile.ZipFile(\"training_data.zip\", \"r\") as zip_ref:\n",
        "  zip_ref.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p29O8MUC-tSa",
        "colab_type": "text"
      },
      "source": [
        "We remove all the images that are not relevant to bike traffic (e.g. highway speed limits) and add reference images for each sign type to the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3bic3pe-KMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not skip_preparation:\n",
        "  !mv GTSRB/Final_Training/Images training_data/\n",
        "  !rm -r training_data/00000\n",
        "  !rm -r training_data/00001\n",
        "  !rm -r training_data/00002\n",
        "  !rm -r training_data/00003\n",
        "  !rm -r training_data/00004\n",
        "  !rm -r training_data/00005\n",
        "  !rm -r training_data/00006\n",
        "  !rm -r training_data/00007\n",
        "  !rm -r training_data/00008\n",
        "  !rm -r training_data/00009\n",
        "  !rm -r training_data/00010\n",
        "  !mv training_data/00011 training_data/priority\n",
        "  !mv training_data/00012 training_data/priority_road\n",
        "  !mv training_data/00013 training_data/yield\n",
        "  !mv training_data/00014 training_data/stop\n",
        "  !mv training_data/00015 training_data/no_entrance\n",
        "  !rm -r training_data/00016\n",
        "  !mv training_data/00017 training_data/no_entry_vehicles\n",
        "  !mv training_data/00018 training_data/danger_ahead\n",
        "  !rm -r training_data/00019\n",
        "  !rm -r training_data/00020\n",
        "  !mv training_data/00021 training_data/double_bend\n",
        "  !rm -r training_data/00022\n",
        "  !rm -r training_data/00023\n",
        "  !rm -r training_data/00024\n",
        "  !mv training_data/00025 training_data/road_works\n",
        "  !mv training_data/00026 training_data/traffic_signals\n",
        "  !mv training_data/00027 training_data/pedestrians_in_road_ahead\n",
        "  !mv training_data/00028 training_data/children_crossing_ahead\n",
        "  !rm -r training_data/00029\n",
        "  !rm -r training_data/00030\n",
        "  !rm -r training_data/00031\n",
        "  !rm -r training_data/00032\n",
        "  !mv training_data/00033 training_data/turn_right_ahead\n",
        "  !mv training_data/00034 training_data/turn_left_ahead\n",
        "  !mv training_data/00035 training_data/proceed_ahead\n",
        "  !mv training_data/00036 training_data/ahead_or_right_only\n",
        "  !mv training_data/00037 training_data/ahead_or_left_only\n",
        "  !mv training_data/00038 training_data/keep_right\n",
        "  !mv training_data/00039 training_data/keep_left\n",
        "  !rm -r training_data/00040\n",
        "  !rm -r training_data/00041\n",
        "  !rm -r training_data/00042\n",
        "  !rm $(find . -iname '*.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw5hf1JOHIDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not skip_preparation:\n",
        "  !wget -q https://www.dvr.de/bilder/stvo/gt/301.png -O training_data/priority/reference.png\n",
        "  !wget -q https://www.dvr.de/bilder/stvo/gt/306.png -O training_data/priority_road/reference.png\n",
        "  !wget -q https://www.dvr.de/bilder/stvo/gt/205.png -O training_data/yield/reference.png\n",
        "  !wget -q https://www.dvr.de/bilder/stvo/gt/206.png -O training_data/stop/reference.png\n",
        "  !wget -q https://www.dvr.de/bilder/stvo/gt/250.png -O training_data/no_entrance/reference.png\n",
        "  !wget -q https://www.dvr.de/bilder/stvo/gt/267.png -O training_data/no_entry_vehicles/reference.png\n",
        "  !wget -q https://www.dvr.de/bilder/stvo/gt/101.png -O training_data/danger_ahead/reference.png\n",
        "  !wget -q https://www.dvr.de/bilder/stvo/gt/105-10.png -O training_data/double_bend/reference.png\n",
        "  !wget -q https://www.dvr.de/bilder/stvo/gt/123.png -O training_data/road_works/reference.png\n",
        "  !wget -q https://www.dvr.de/bilder/stvo/gt/131.png -O training_data/traffic_signals/reference.png\n",
        "  !wget -q https://www.dvr.de/bilder/stvo/gt/133-10.png -O training_data/pedestrians_in_road_ahead/reference.png\n",
        "  !wget -q https://www.dvr.de/bilder/stvo/gt/136-10.png -O training_data/children_crossing_ahead/reference.png\n",
        "  !wget -q https://www.dvr.de/bilder/stvo/gt/209.png -O training_data/turn_right_ahead/reference.png\n",
        "  !wget -q https://www.dvr.de/bilder/stvo/gt/214.png -O training_data/ahead_or_right_only/reference.png\n",
        "  !wget -q https://www.dvr.de/bilder/stvo/gt/222.png -O training_data/keep_right/reference.png"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R72X5AWJFtQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not skip_preparation:\n",
        "  cv.imwrite(\"training_data/turn_left_ahead/reference.png\", cv.flip(cv.imread(\"training_data/turn_right_ahead/reference.png\"), 1))\n",
        "  cv.imwrite(\"training_data/keep_left/reference.png\", cv.flip(cv.imread(\"training_data/keep_right/reference.png\"), 1))\n",
        "  cv.imwrite(\"training_data/ahead_or_left_only/reference.png\", cv.flip(cv.imread(\"training_data/ahead_or_right_only/reference.png\"), 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r_edL7-TT8Gx"
      },
      "source": [
        "We can take a look at the extracted training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BmG-92UcT8Gy",
        "colab": {}
      },
      "source": [
        "!apt install -qq tree > /dev/null\n",
        "!tree --filelimit 20 training_data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0vYzgXe9T8G3"
      },
      "source": [
        "It consists of 19 categories of traffic signs that have been extracted from the GTSRB dataset. Each image shows exactly one traffic sign, usually roughly at the center of the image. Some of the traffic signs might be partially ocluded, rotated, or facing away from the camera."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "luCxSgs6T8G3",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "fig = plt.figure(figsize=(20,35))\n",
        "for sign_num, sign_type in enumerate(os.listdir(\"training_data/\")):\n",
        "    sign_path = os.path.join(\"training_data\", sign_type)\n",
        "    images = np.array(os.listdir(sign_path))\n",
        "    \n",
        "    selector = np.random.randint(0, len(sign_path), size=3)\n",
        "    \n",
        "    for j, img in enumerate(images[selector]):\n",
        "        plt.subplot(20, 3, (sign_num * 3) + j + 1)\n",
        "        plt.xlabel(sign_type)\n",
        "        plt.imshow(cv.cvtColor(cv.imread(os.path.join(sign_path, img)), cv.COLOR_BGR2RGB))\n",
        "        \n",
        "fig.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UQk_MRj9T8HB"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Or7EHv_ZT8HC"
      },
      "source": [
        "In order to prevent overfitting in our machine learning models, we make use of image augmentation in order to extend the training set size.\n",
        "In a first step, we convert the images to a suitable file format for further processing and remove those images that are too small to be usable (below 50 x 50 px in size)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pmaZwJNHxjV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not skip_preparation:\n",
        "  for sign_num, sign_type in enumerate(os.listdir(\"training_data/\")):\n",
        "      sign_path = os.path.join(\"training_data\", sign_type)\n",
        "      images = os.listdir(sign_path)\n",
        "      \n",
        "      for image in images:\n",
        "        try:\n",
        "          img = cv.imread(os.path.join(sign_path, image))\n",
        "\n",
        "          if img.shape[0] < 50 or img.shape[1] < 50:\n",
        "              os.remove(os.path.join(sign_path, image))\n",
        "          \n",
        "          elif image.endswith(\"ppm\"):\n",
        "              cv.imwrite(os.path.join(sign_path, image.replace(\"ppm\", \"png\")), img)\n",
        "              os.remove(os.path.join(sign_path, image))\n",
        "        except:\n",
        "          pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7UtbdYiWvq0",
        "colab_type": "text"
      },
      "source": [
        "We can run an image augmentation library on the extracted data. Note that this takes a very long time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OfsBaICiWj5H",
        "colab": {}
      },
      "source": [
        "if not skip_preparation:\n",
        "  TARGET_NUMBER_OF_IMGS = 2000\n",
        "\n",
        "  for sign_num, sign_type in enumerate(os.listdir(\"training_data/\")):\n",
        "      sign_path = os.path.join(\"training_data\", sign_type)\n",
        "      num_images = len(os.listdir(sign_path))\n",
        "\n",
        "      p = Augmentor.Pipeline(sign_path)\n",
        "      print(\"\\n\")\n",
        "\n",
        "      p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n",
        "      p.zoom(probability=0.5, min_factor=1.1, max_factor=1.5)\n",
        "      p.skew(probability=0.3)\n",
        "\n",
        "      if num_images <= TARGET_NUMBER_OF_IMGS:\n",
        "        p.sample(TARGET_NUMBER_OF_IMGS - num_images)\n",
        "      \n",
        "      print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABGgsWqWswe8",
        "colab_type": "text"
      },
      "source": [
        "We can now move the generated images into the correct directories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2nheQRpsTsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not skip_preparation:\n",
        "  for sign_num, sign_type in enumerate(os.listdir(\"training_data/\")):\n",
        "      sign_path = os.path.join(\"training_data\", sign_type)\n",
        "      output_path = os.path.join(sign_path, \"output\")\n",
        "\n",
        "      for filename in os.listdir(output_path):\n",
        "        os.rename(os.path.join(output_path, filename), os.path.join(sign_path, filename))\n",
        "\n",
        "      os.rmdir(os.path.join(sign_path, \"output\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "plEtlzgtT8HF"
      },
      "source": [
        "The data is processed by loading all images, resizing them to a dimension of 100 by 100 pixels and then storing the images in a NumPy array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3ii9YXklT8HH",
        "colab": {}
      },
      "source": [
        "def process_image(fileName):\n",
        "    img = cv.imread(fileName)\n",
        "    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
        "    img = cv.resize(img, (100,100))\n",
        "    return img\n",
        "\n",
        "DATA_PATH = \"training_data/\"\n",
        "\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "num_samples = 0\n",
        "\n",
        "signs = os.listdir(DATA_PATH)\n",
        "\n",
        "for sign in signs:\n",
        "    current_samples = num_samples\n",
        "    \n",
        "    sign_path = os.path.join(DATA_PATH, sign)\n",
        "    \n",
        "    for file in os.listdir(sign_path):\n",
        "        if not (file.endswith(\".ppm\") or file.endswith(\".png\")):\n",
        "            continue\n",
        "\n",
        "        if num_samples - current_samples > 2000:\n",
        "          continue\n",
        "        \n",
        "        img = process_image(os.path.join(sign_path, file))\n",
        "        \n",
        "        X_train.append(img)\n",
        "        y_train.append(signs.index(sign))\n",
        "        num_samples += 1\n",
        "\n",
        "    print(f\"Read {num_samples - current_samples} samples for {sign}.\")\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "print(f\"Number of Samples: {num_samples}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr24f31WXA2g",
        "colab_type": "text"
      },
      "source": [
        "The authors of the GTSRB dataset provide an additional test dataset to be used during the development of the model. We download the files (one zip file containing the images and another one containing the labels)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7nULi11XD1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -c https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_Images.zip -O test_data.zip\n",
        "!wget -c https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_GT.zip -O test_data_annotation.zip\n",
        "\n",
        "with zipfile.ZipFile(\"test_data.zip\", \"r\") as zip_ref:\n",
        "  zip_ref.extractall(\"test_data\")\n",
        "\n",
        "with zipfile.ZipFile(\"test_data_annotation.zip\", \"r\") as zip_ref:\n",
        "  zip_ref.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20aqKhdRXHwp",
        "colab_type": "text"
      },
      "source": [
        "The dataset labels are provided in a CSV file, which we read using pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "425xKZS4XGOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas\n",
        "\n",
        "test_files = pandas.read_csv(\"GT-final_test.csv\", sep=';')\n",
        "test_files.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jdk-XroRX5WZ",
        "colab_type": "text"
      },
      "source": [
        "We need a way to map the ClassId column in the CSV file back to the sign categories we are working with. We do this by creating a map that maps all valid ClassIds to the name of the sign, and maps all other ClassIds to -1 so that we can ignore them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uKSIhzGX6cc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_mapping = {\n",
        "   0: -1,\n",
        "   1: -1,\n",
        "   2: -1,\n",
        "   3: -1,\n",
        "   4: -1,\n",
        "   5: -1,\n",
        "   6: -1,\n",
        "   7: -1,\n",
        "   8: -1,\n",
        "   9: -1,\n",
        "   10: -1,\n",
        "   11: \"priority\",\n",
        "   12: \"priority_road\",\n",
        "   13: \"yield\",\n",
        "   14: \"stop\",\n",
        "   15: \"no_entrance\",\n",
        "   16: -1,\n",
        "   17: \"no_entry_vehicles\",\n",
        "   18: \"danger_ahead\",\n",
        "   19: -1,\n",
        "   20: -1,\n",
        "   21: \"double_bend\",\n",
        "   22: -1,\n",
        "   23: -1,\n",
        "   24: -1,\n",
        "   25: \"road_works\",\n",
        "   26: \"traffic_signals\",\n",
        "   27: \"pedestrians_in_road_ahead\",\n",
        "   28: \"children_crossing_ahead\",\n",
        "   29: -1,\n",
        "   30: -1,\n",
        "   31: -1,\n",
        "   32: -1,\n",
        "   33: \"turn_right_ahead\",\n",
        "   34: \"turn_left_ahead\",\n",
        "   35: \"proceed_ahead\",\n",
        "   36: \"ahead_or_right_only\",\n",
        "   37: \"ahead_or_left_only\",\n",
        "   38: \"keep_right\",\n",
        "   39: \"keep_left\",\n",
        "   40: -1,\n",
        "   41: -1,\n",
        "   42: -1\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wELpKmIZX9iu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_PATH = \"test_data/GTSRB/Final_Test/Images/\"\n",
        "\n",
        "X_test = []\n",
        "y_test = []\n",
        "\n",
        "for _, test_file in test_files.iterrows():\n",
        "    imgpath = os.path.join(DATA_PATH, test_file[\"Filename\"])\n",
        "\n",
        "    classid = class_mapping[int(test_file[\"ClassId\"])]\n",
        "\n",
        "    if classid == -1 or test_file[\"Width\"] <= 50 or test_file[\"Height\"] <= 50:\n",
        "      continue\n",
        "    \n",
        "    img = process_image(imgpath)\n",
        "    \n",
        "    X_test.append(img)\n",
        "    y_test.append(signs.index(classid))\n",
        "\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f58_Pe9X_jL",
        "colab_type": "text"
      },
      "source": [
        "This yields the following distribution for the training and test sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfiK93tuYC1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(40,10))\n",
        "plt.bar(signs, [len(y_train[y_train == i]) for i in range(0, len(signs))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSAB8SgXYLmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(40,10))\n",
        "plt.bar(signs, [len(y_test[y_test == i]) for i in range(0, len(signs))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLQKyCqkYNwE",
        "colab_type": "text"
      },
      "source": [
        "As a final validation set we use the test data that was used to evaluate contributions to the IJCNN 2011 competition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od2SSaWtYQM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -c https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Online-Test-Images-Sorted.zip -O test_data_final.zip\n",
        "\n",
        "!rm -r test_data_final\n",
        "with zipfile.ZipFile(\"test_data_final.zip\", \"r\") as zip_ref:\n",
        "  zip_ref.extractall(\"test_data_final\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IPOTNurYUKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r test_data_final/GTSRB/Online-Test-sort/00000\n",
        "!rm -r test_data_final/GTSRB/Online-Test-sort/00001\n",
        "!rm -r test_data_final/GTSRB/Online-Test-sort/00002\n",
        "!rm -r test_data_final/GTSRB/Online-Test-sort/00003\n",
        "!rm -r test_data_final/GTSRB/Online-Test-sort/00004\n",
        "!rm -r test_data_final/GTSRB/Online-Test-sort/00005\n",
        "!rm -r test_data_final/GTSRB/Online-Test-sort/00006\n",
        "!rm -r test_data_final/GTSRB/Online-Test-sort/00007\n",
        "!rm -r test_data_final/GTSRB/Online-Test-sort/00008\n",
        "!rm -r test_data_final/GTSRB/Online-Test-sort/00009\n",
        "!rm -r test_data_final/GTSRB/Online-Test-sort/00010\n",
        "!mv test_data_final/GTSRB/Online-Test-sort/00011 test_data_final/GTSRB/Online-Test-sort/priority\n",
        "!mv test_data_final/GTSRB/Online-Test-sort/00012 test_data_final/GTSRB/Online-Test-sort/priority_road\n",
        "!mv test_data_final/GTSRB/Online-Test-sort/00013 test_data_final/GTSRB/Online-Test-sort/yield\n",
        "!mv test_data_final/GTSRB/Online-Test-sort/00014 test_data_final/GTSRB/Online-Test-sort/stop\n",
        "!mv test_data_final/GTSRB/Online-Test-sort/00015 test_data_final/GTSRB/Online-Test-sort/no_entrance\n",
        "!rm -r test_data_final/GTSRB/Online-Test-sort/00016\n",
        "!mv test_data_final/GTSRB/Online-Test-sort/00017 test_data_final/GTSRB/Online-Test-sort/no_entry_vehicles\n",
        "!mv test_data_final/GTSRB/Online-Test-sort/00018 test_data_final/GTSRB/Online-Test-sort/danger_ahead\n",
        "!rm -r test_data_final/GTSRB/Online-Test-sort/00019\n",
        "!rm -r test_data_final/GTSRB/Online-Test-sort/00020\n",
        "!mv test_data_final/GTSRB/Online-Test-sort/00021 test_data_final/GTSRB/Online-Test-sort/double_bend\n",
        "!rm -r test_data_final/GTSRB/Online-Test-sort/00022\n",
        "!rm -r test_data_final/GTSRB/Online-Test-sort/00023\n",
        "!rm -r test_data_final/GTSRB/Online-Test-sort/00024\n",
        "!mv test_data_final/GTSRB/Online-Test-sort/00025 test_data_final/GTSRB/Online-Test-sort/road_works\n",
        "!mv test_data_final/GTSRB/Online-Test-sort/00026 test_data_final/GTSRB/Online-Test-sort/traffic_signals\n",
        "!mv test_data_final/GTSRB/Online-Test-sort/00027 test_data_final/GTSRB/Online-Test-sort/pedestrians_in_road_ahead\n",
        "!mv test_data_final/GTSRB/Online-Test-sort/00028 test_data_final/GTSRB/Online-Test-sort/children_crossing_ahead\n",
        "!rm -r test_data_final/GTSRB/Online-Test-sort/00029\n",
        "!rm -r test_data_final/GTSRB/Online-Test-sort/00030\n",
        "!rm -r test_data_final/GTSRB/Online-Test-sort/00031\n",
        "!rm -r test_data_final/GTSRB/Online-Test-sort/00032\n",
        "!mv test_data_final/GTSRB/Online-Test-sort/00033 test_data_final/GTSRB/Online-Test-sort/turn_right_ahead\n",
        "!mv test_data_final/GTSRB/Online-Test-sort/00034 test_data_final/GTSRB/Online-Test-sort/turn_left_ahead\n",
        "!mv test_data_final/GTSRB/Online-Test-sort/00035 test_data_final/GTSRB/Online-Test-sort/proceed_ahead\n",
        "!mv test_data_final/GTSRB/Online-Test-sort/00036 test_data_final/GTSRB/Online-Test-sort/ahead_or_right_only\n",
        "!mv test_data_final/GTSRB/Online-Test-sort/00037 test_data_final/GTSRB/Online-Test-sort/ahead_or_left_only\n",
        "!mv test_data_final/GTSRB/Online-Test-sort/00038 test_data_final/GTSRB/Online-Test-sort/keep_right\n",
        "!mv test_data_final/GTSRB/Online-Test-sort/00039 test_data_final/GTSRB/Online-Test-sort/keep_left\n",
        "!rm -r test_data_final/GTSRB/Online-Test-sort/00040\n",
        "!rm -r test_data_final/GTSRB/Online-Test-sort/00041\n",
        "!rm -r test_data_final/GTSRB/Online-Test-sort/00042\n",
        "!rm -r test_data_final/GTSRB/Online-Test-sort/Images\n",
        "!rm -r test_data_final/GTSRB/Online-Test-sort/Readme.txt\n",
        "!rm $(find test_data_final/GTSRB/Online-Test-sort/ -iname '*.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zL6CnH-YWFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_PATH = \"test_data_final/GTSRB/Online-Test-sort/\"\n",
        "\n",
        "X_validation = []\n",
        "y_validation = []\n",
        "\n",
        "validation_images = os.listdir(DATA_PATH)\n",
        "\n",
        "for vimgs in validation_images:\n",
        "    vimgs_path = os.path.join(DATA_PATH, vimgs)\n",
        "    \n",
        "    for file in os.listdir(vimgs_path):\n",
        "        if not (file.endswith(\".ppm\") or file.endswith(\".png\")):\n",
        "            continue\n",
        "        \n",
        "        shape = cv.imread(os.path.join(vimgs_path, file)).shape\n",
        "\n",
        "        if shape[0] < 50 or shape[1] < 50:\n",
        "          continue\n",
        "\n",
        "        img = process_image(os.path.join(vimgs_path, file))\n",
        "        \n",
        "        X_validation.append(img)\n",
        "        y_validation.append(signs.index(vimgs))\n",
        "\n",
        "X_validation = np.array(X_validation)\n",
        "y_validation = np.array(y_validation)\n",
        "\n",
        "print(X_validation.shape)\n",
        "print(y_validation.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9BkEAA2YW5U",
        "colab_type": "text"
      },
      "source": [
        "We now have a training set used to train our models and two test sets used to evaluate their performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u7MaCimzT8HM"
      },
      "source": [
        "## Extracting image features and learning using a support vector machine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NcFcuTpjT8HM"
      },
      "source": [
        "Our first approach to classifiying the traffic sign images, is using a support vector machine [0] classifier. In order to do that, we first compute a feature vector for each image by computing Histogram of Oriented Gradients (HOG) [1] features for each image.\n",
        "\n",
        "[0] James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani, eds. An Introduction to Statistical Learning: With Applications in R. Springer Texts in Statistics 103. New York: Springer, 2013.\n",
        "\n",
        "[1] https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yXG7pFmGT8HN",
        "colab": {}
      },
      "source": [
        "import skimage.feature\n",
        "\n",
        "X_train_hog = np.array([skimage.feature.hog(img) for img in X_train])\n",
        "X_test_hog = np.array([skimage.feature.hog(img) for img in X_test])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LBfWQ8hjT8HP"
      },
      "source": [
        "After the conversion the training set now consists of *n* feature vectors, each with a size of 5184 predictors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GQNCAFGAT8HP",
        "colab": {}
      },
      "source": [
        "X_train_hog.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pZlU_oxNT8HR"
      },
      "source": [
        "We now have converted from the original high-dimensional space of dimension `19200 (= 80 * 80 * 3)` to a 5184-dimensional space, which is much more anemable to analysis by a statistical learning algorithm.\n",
        "\n",
        "We can now use the `SGDClassifier` provided by scikit-learn, which implements a support vector machine trained using stochastic gradient descent (SGD)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kxSLma7bWj4e",
        "colab": {}
      },
      "source": [
        "from sklearn import linear_model\n",
        "\n",
        "clf = linear_model.SGDClassifier(max_iter=100, tol=1e-3, n_jobs=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4Jr3KTVBT8HS"
      },
      "source": [
        "We can now fit the classifier to the training set of HOG feature vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4rCOnZGTT8HT",
        "colab": {}
      },
      "source": [
        "clf.fit(X_train_hog, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HUYPvWRNT8HV"
      },
      "source": [
        "We can now compute the training and test accuracy of our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U_8v4DOnT8HW",
        "colab": {}
      },
      "source": [
        "print(f\"Training accuracy: {clf.score(X_train_hog, y_train)}\")\n",
        "print(f\"Test accuracy: {clf.score(X_test_hog, y_test)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZhhlor5Yl2I",
        "colab_type": "text"
      },
      "source": [
        "Although this model provides adequate accuracy, it suffers from poor runtime performance due to the expensive computation of the HOG feature vectors. We have therefore focused on an alternative approach using a convolutional neural network (CNN) described below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VjlLd7NYWj5G"
      },
      "source": [
        "## Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gjv3kEgXWj5J",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import tensorflow\n",
        "\n",
        "from tensorflow.keras import models, layers, optimizers\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "\n",
        "tensorflow.get_logger().setLevel('ERROR')\n",
        "\n",
        "#tensorflow.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jztcjan9ZEIi",
        "colab_type": "text"
      },
      "source": [
        "The model we designed is roughly guided by the VGG16 architecture [1] and the description of CNNs for image recognition given in the book by Chollet [2].\n",
        "As such, it consists of two main components: A feature extractor build using convolutional layers and a classifier using fully-connected (i.e. dense) layers.\n",
        "The feature extractor is build using 3 blocks of convolutional layers followed by Dropout for regularization and max pooling to reduce the input dimensions.\n",
        "The classification happens at the last stage of the network, where the fully-connected \"head\" classifies the result into the traffic sign classes using a fully-connected layer with a softmax activation.\n",
        "\n",
        "[1] Simonyan, Karen and Andrew Zisserman. âVery Deep Convolutional Networks for Large-Scale Image Recognition.â CoRR abs/1409.1556 (2014)\n",
        "\n",
        "[2] Chollet, FranÃ§ois. âDeep Learning with Python.â (2017)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xYvCDIBFWj5K",
        "colab": {}
      },
      "source": [
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(32, (3, 3), input_shape=(100, 100, 3)))\n",
        "model.add(layers.Dropout(0.1))\n",
        "model.add(layers.MaxPool2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3)))\n",
        "model.add(layers.Dropout(0.25))\n",
        "model.add(layers.MaxPool2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(128, (3, 3)))\n",
        "model.add(layers.Dropout(0.25))\n",
        "model.add(layers.MaxPool2D((2, 2)))\n",
        "\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dense(1024, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "model.add(layers.Dense(20, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viltK_UAaQ6-",
        "colab_type": "text"
      },
      "source": [
        "This is what our models looks like, including the number of parameters and the output shapes of the individual layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Na73widKT8Hc",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XJZ7GQqaZF7",
        "colab_type": "text"
      },
      "source": [
        "We also make use of label smoothing [1], which is provided out of the box by Keras, to help prevent overfitting.\n",
        "\n",
        "[1] He, Tong et al. âBag of Tricks for Image Classification with Convolutional Neural Networks.â 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2018): 558-567."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZvR7bRivdHpw",
        "colab": {}
      },
      "source": [
        "model.compile(loss=CategoricalCrossentropy(label_smoothing=0.2), \n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fpWryJyWdOzm",
        "colab": {}
      },
      "source": [
        "X_train_reshaped = (X_train.astype('float32') / 255.0)\n",
        "X_test_reshaped = X_test.astype('float32') / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6K5QLal7auMz",
        "colab_type": "text"
      },
      "source": [
        "We train our model using 30 epochs and include the test data as `validation_data` so that we can see the test set performance during the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vw92BHzLY71U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train_reshaped, to_categorical(y_train), \n",
        "                    validation_data=(X_test_reshaped, to_categorical(y_test)), \n",
        "                    epochs=30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib8Ij8zPY_PK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc'] \n",
        "loss = history.history['loss'] \n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc') \n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc') \n",
        "plt.title('Training and validation accuracy') \n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss') \n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss') \n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kazPvxKGeCnY",
        "colab": {}
      },
      "source": [
        "model.evaluate(X_test_reshaped, to_categorical(y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE0Zfqt9bCs1",
        "colab_type": "text"
      },
      "source": [
        "In addition to the raw accuracy, we can take a look at the confusion matrix to see if and where our model makes classification errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05g4OJQr9__3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, [np.argmax(p) for p in model.predict((X_test.astype('float32') / 255.0))])\n",
        "\n",
        "print(np.array_str(cm, max_line_width=120))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NK4S7e2v7FN",
        "colab_type": "text"
      },
      "source": [
        "## Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvtfIqTsv7FN",
        "colab_type": "text"
      },
      "source": [
        "As described in the previous section, using a CNN trained from scratch we are able to achieve an accuracy of about 92% on the test set. As a second approach we tried to improve upon that by using transfer learning. As a base architecture we use the VGG16 network, which comes prepackaged with Keras.\n",
        "\n",
        "The model was trained on the ImageNet dataset and can be loaded (as we do here)\n",
        "without the fully-connected classifier layers so that we can train our own classifier by using the VGG16 base network as a feature extractor and classifying the features using a fully-connected classifier on top."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GrBq61lv7FO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "vgg_base = VGG16(weights='imagenet',\n",
        "                 include_top=False,\n",
        "                 input_shape=(100, 100, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlaSUtogv7FP",
        "colab_type": "text"
      },
      "source": [
        "We use the existing model as a feature extractor and add our dense layers on top to do the classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9_YyQ28v7FQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vgg_base.trainable = True\n",
        "\n",
        "for layer in vgg_base.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "vgg_base.get_layer(name=\"block5_conv1\").trainable = True\n",
        "vgg_base.get_layer(name=\"block5_conv2\").trainable = True\n",
        "vgg_base.get_layer(name=\"block5_conv3\").trainable = True\n",
        "\n",
        "transfer_model = models.Sequential()\n",
        "\n",
        "transfer_model.add(vgg_base)\n",
        "\n",
        "transfer_model.add(layers.Dropout(0.5))\n",
        "transfer_model.add(layers.Flatten()) \n",
        "transfer_model.add(layers.Dense(2048, activation='relu'))\n",
        "transfer_model.add(layers.Dropout(0.5))\n",
        "transfer_model.add(layers.Dense(20, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV92zW3wv7FR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transfer_model.compile(loss=CategoricalCrossentropy(label_smoothing=0.2), \n",
        "                       optimizer=optimizers.RMSprop(lr=1e-5),\n",
        "                       metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpHB81vpv7FS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transfer_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WCyfs-Dbff0",
        "colab_type": "text"
      },
      "source": [
        "As the summary shows, the VGG network is almost twice as large as our own CNN. Since we have frozen almost all of the base networks layers (i.e. they won't be updated during training) we only have to adjust the rougly 7 million parameters in the remaining layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wA75027Hv7FT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = transfer_model.fit(X_train_reshaped, to_categorical(y_train), \n",
        "                            validation_data=(X_test_reshaped, to_categorical(y_test)), \n",
        "                            epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m5xL2txbjnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc'] \n",
        "loss = history.history['loss'] \n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc') \n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc') \n",
        "plt.title('Training and validation accuracy') \n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss') \n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss') \n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26l60LVAcfpU",
        "colab_type": "text"
      },
      "source": [
        "Using this model we are now able to achieve 96% accuracy on the test set provided by the GTSRB authors, thus improving on the previous model we trained from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3b0HogiLu0d",
        "colab_type": "text"
      },
      "source": [
        "## Import and analyse a Picture\n",
        "\n",
        "This is our final implementation for the analysis of an image. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdhSrBA06uJJ",
        "colab_type": "text"
      },
      "source": [
        "First of all, we need the desired output characters to present the result of the analysis with a recommendation for action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RimXkU2jEX32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q 'https://raw.githubusercontent.com/uol-mediaprocessing/group-projects-traffic-signs/master/Pres_5/img/stop.jpg'  -O stop.jpg\n",
        "!wget -q 'https://raw.githubusercontent.com/uol-mediaprocessing/group-projects-traffic-signs/master/Pres_5/img/thump_up_green.jpg' -O thump_up_green.jpg\n",
        "!wget -q 'https://raw.githubusercontent.com/uol-mediaprocessing/group-projects-traffic-signs/master/Pres_5/img/thump_up_yellow.jpg' -O thump_up_yellow.jpg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5ZOXmG6lPD6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_w = 0\n",
        "start_h = 0\n",
        "end_w = 0\n",
        "end_h = 0\n",
        "def get_range_contours(x, y, w, h, image_h, image_w):\n",
        "  mid_w = int(x + w/2)\n",
        "  mid_h = int(y + h/2)\n",
        "  if w < h:\n",
        "    h = int((h + (h * 0.5))/2)\n",
        "    w = h\n",
        "  elif w > h:\n",
        "    w = int((w + (w * 0.5))/2)\n",
        "    h = w\n",
        "  start_h = np.maximum(mid_h - h, 0)\n",
        "  end_h = np.minimum(mid_h + h, image_h-1)\n",
        "  start_w = np.maximum(mid_w - w, 0)\n",
        "  end_w = np.minimum(mid_w + w, image_w-1)\n",
        "  return start_h,end_h, start_w, end_w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u0kQPvk9H_o",
        "colab_type": "text"
      },
      "source": [
        "The following method examines the created mask for contours. Only contours with a certain size, depending on the overall size of the image, are considered. If an outline is detected, it is cut out and labelled by our Machine Learning Algorythm. Based on the suspected traffic sign, an index for the respective recommended action is stored. In the end, it shows the contour and calls up the method for pointing out the recommended action with the stored index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTQJ_4vyz3Ql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_contours(image,mask):\n",
        "  (cnts, _) = cv.findContours(mask.copy(), cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "  idx = 0\n",
        "  contour_images = []\n",
        "  contour_titles = []\n",
        "  bimage = image.copy()\n",
        "  image_size = image.shape[0] * image.shape[1] * 0.001\n",
        "  result_sign = 0\n",
        "  for c in cnts:\n",
        "    area = cv.contourArea(c) \n",
        "    # Shortlisting the regions based on there area. \n",
        "    if area > image_size:\n",
        "      x,y,w,h = cv.boundingRect(c)\n",
        "      if w>50 and h>50:\n",
        "        idx+=1\n",
        "        (sh,eh,sw,ew) = get_range_contours(x,y,w,h,image.shape[0],image.shape[1])\n",
        "        new_img = image[sh:eh,sw:ew]\n",
        "        cv.imwrite(str(idx) + '.png', new_img)\n",
        "        sign_image = cv.resize(new_img, (100, 100))\n",
        "        sign_image = (sign_image.astype('float32') / 255)\n",
        "        contour_images.append(new_img)\n",
        "        pred = transfer_model.predict([sign_image])\n",
        "        label = signs[np.argmax(pred)]\n",
        "        result_sign = check_result(np.argmax(pred), result_sign)\n",
        "        contour_titles.append(label)\n",
        "        # print(f\"label: {label} {pred}\")\n",
        "  \n",
        "        cv.putText(bimage, label, (x, y), cv.FONT_HERSHEY_COMPLEX, 1.5, (255, 0, 0), 2 , cv.LINE_AA)\n",
        "        bimage = cv.rectangle(bimage, (sw, sh), (ew, eh), (255,0,0), 2)\n",
        "\n",
        "  show_images(contour_images,1,contour_titles) \n",
        "  show_result_sign(result_sign)\n",
        "  #plt.figure(figsize=(20,20))\n",
        "  #plt.imshow(bimage)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4ErorkQ-Ryo",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S41Lk3TGY_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_result(sign_number, previous):\n",
        "  if previous == 1 or (sign_number == 1 or sign_number == 3 or sign_number == 6 or sign_number == 8):\n",
        "    return 1\n",
        "  elif (sign_number == 13 or sign_number == 16):\n",
        "    return 2\n",
        "  else:\n",
        "    return previous"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMjH9RRp_ygB",
        "colab_type": "text"
      },
      "source": [
        "Depending on the index of the recommended action, the respective image is loaded and output for visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShhuqUDiLTVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_result_sign(sign_num):\n",
        "  plt.figure()\n",
        "  if sign_num == 1:\n",
        "    imgre = cv.imread(\"stop.jpg\")\n",
        "  elif sign_num == 2:\n",
        "    imgre = cv.imread(\"thump_up_green.jpg\")\n",
        "  else:\n",
        "    imgre = cv.imread(\"thump_up_yellow.jpg\")\n",
        "  imgre=cv.cvtColor(imgre, cv.COLOR_BGR2RGB)\n",
        "  show_images([imgre],1,[\"result\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KnItncMAFuu",
        "colab_type": "text"
      },
      "source": [
        "**Execution** \n",
        "\n",
        "The first cell allows the user to upload an image for analysis. The second analyzes the image using the HSV color space used at the beginning. (Only the color red is filtered)\n",
        "The third method analyses the image by filtering for red, blue and yellow. Both methods give a visual recommendation for action as feedback. We have thus achieved our goal of recognizing traffic signs in good conditions in the scene and giving a child-oriented recommendation for action.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw79YOdkL-ZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48eFxovgfFVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for fn in uploaded.keys():\n",
        "  load_image(fn)\n",
        "  image_return = image_transformation_hsv(fn)\n",
        "  get_contours(image_return[0], image_return[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUQrh-A4hN3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for fn in uploaded.keys():\n",
        "  load_image(fn)\n",
        "  image_return = image_transformation_hls(fn)\n",
        "  get_contours(image_return[0], image_return[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzKnx4bzW2tQ",
        "colab_type": "text"
      },
      "source": [
        "# 6. Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWfzY_G8cL6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
