{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"main_traffic-signs.ipynb","provenance":[{"file_id":"https://github.com/uol-mediaprocessing/group-projects-traffic-signs/blob/master/Copy%20of%20Copy%20of%20try2_with_hls.ipynb","timestamp":1580923146773},{"file_id":"https://github.com/uol-mediaprocessing/group-projects-traffic-signs/blob/master/Pres_5/Copy%20of%20try2_with_hls.ipynb","timestamp":1579593409650},{"file_id":"https://github.com/uol-mediaprocessing/group-projects-traffic-signs/blob/master/Pres_5/try2_with_hls.ipynb","timestamp":1576512892623},{"file_id":"https://github.com/uol-mediaprocessing/group-projects-traffic-signs/blob/master/%5CPres_5%5Ctry.ipynb","timestamp":1575887882693}],"collapsed_sections":["9kQcKta5U9XB","VhuhtLmlTD4D","z0uE5MM-46XU","pKwyVJGemcpx","h9JwskgFWj4e","u7MaCimzT8HM","VjlLd7NYWj5G","gzKnx4bzW2tQ","e6czrwtbT8Ho","NAwr4eGVZuum"],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9kQcKta5U9XB"},"source":["#  Assisting children while biking\n","\n","### Gruppe 7: Maximilian HÃ¶rnis, Florian Schwarm, Dennis Rupprecht"]},{"cell_type":"markdown","metadata":{"id":"SA3gWpqGQrSP","colab_type":"text"},"source":["# Table of Content\n","## 1. Introduction\n","## 2. Imports\n","## 3. Color recognition\n","## 4. Classification by Contours\n","## 5. Machine Learning\n","## 6. Conclusion"]},{"cell_type":"markdown","metadata":{"id":"VhuhtLmlTD4D","colab_type":"text"},"source":["# 1. Introduction"]},{"cell_type":"markdown","metadata":{"id":"5uGNSSQYTAgs","colab_type":"text"},"source":["## Motivation \n","- Children are active participants in road traffic\n","- Traffic signs might not be obvious to them\n","- Handheld technology device (smartphones etc.) usage is prevalent among children "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BPiTRiyMUyjp"},"source":["## Idea\n","\n","Recognize and classify traffic signs automatically and display them in\n","an easy to understand assistive technology for children, without distracting them."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0MIfdy9bUyjs"},"source":["## Tasks\n","    \n","- Recognizing and classifiying traffic signs\n","- Interpretation of images in a way that is\n","    - a) sufficiently comprehensible for children and\n","    - b) not distracting."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"12OZ1BgyUyju"},"source":["## Possible Extensions\n","\n","- Detection of intersections without relying on traffic signs\n","- Detection of traffic lights and their current position (red, green, yellow)\n","- Applying the traffic sign classifier to live video (or recordings)\n","- Integrating the system into a mobile application"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VSqMLdJ7Uyjx"},"source":["## Possible Issues\n","\n","- Recognizing dirty or occluded signs\n","\n","- Determining which street a sign refers to and how long it is valid\n","- Reliable detection of intersections"]},{"cell_type":"markdown","metadata":{"id":"z0uE5MM-46XU","colab_type":"text"},"source":["# 2. Imports"]},{"cell_type":"code","metadata":{"id":"NtEKBaAz4zLI","colab_type":"code","outputId":"7975ea6a-2dbe-4673-b12b-6d76f4f4baa5","executionInfo":{"status":"ok","timestamp":1580923142610,"user_tz":-60,"elapsed":4354,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install Augmentor"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: Augmentor in /usr/local/lib/python3.6/dist-packages (0.2.8)\n","Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from Augmentor) (0.16.0)\n","Requirement already satisfied: tqdm>=4.9.0 in /usr/local/lib/python3.6/dist-packages (from Augmentor) (4.28.1)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from Augmentor) (1.17.5)\n","Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from Augmentor) (6.2.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ddr7z8iW4-bs","colab_type":"code","colab":{}},"source":["import zipfile\n","import cv2 as cv\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os.path\n","import Augmentor\n","import os"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pKwyVJGemcpx","colab_type":"text"},"source":["# 3. Color recognition"]},{"cell_type":"code","metadata":{"id":"mw5BuWvti8wu","colab_type":"code","colab":{}},"source":["# Load a image\n","def load_image(image_name):\n","  image = cv.imread(image_name)\n","  image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n","  img_hsv = cv.cvtColor(image, cv.COLOR_RGB2HSV)\n","  fig = plt.figure()\n","  plt.axis(\"off\")\n","  plt.imshow(image)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RimXkU2jEX32","colab_type":"code","colab":{}},"source":["!wget -q 'https://raw.githubusercontent.com/uol-mediaprocessing/group-projects-traffic-signs/master/Pres_5/img/stop.jpg'  -O stop.jpg\n","!wget -q 'https://raw.githubusercontent.com/uol-mediaprocessing/group-projects-traffic-signs/master/Pres_5/img/thump_up_green.jpg' -O thump_up_green.jpg\n","!wget -q 'https://raw.githubusercontent.com/uol-mediaprocessing/group-projects-traffic-signs/master/Pres_5/img/thump_up_yellow.jpg' -O thump_up_yellow.jpg"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3S41Lk3TGY_r","colab_type":"code","colab":{}},"source":["def check_result(sign_number, previous):\n","  if previous == 1 or (sign_number == 1 or sign_number == 3 or sign_number == 6 or sign_number == 8):\n","    return 1\n","  elif (sign_number == 13 or sign_number == 16):\n","    return 2\n","  else:\n","    return previous"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ShhuqUDiLTVN","colab_type":"code","colab":{}},"source":["def show_result_sign(sign_num):\n","  plt.figure()\n","  if sign_num == 1:\n","    imgre = cv.imread(\"stop.jpg\")\n","  elif sign_num == 2:\n","    imgre = cv.imread(\"thump_up_green.jpg\")\n","  else:\n","    imgre = cv.imread(\"thump_up_yellow.jpg\")\n","  imgre=cv.cvtColor(imgre, cv.COLOR_BGR2RGB)\n","  show_images([imgre],1,[\"result\"])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M_kxJ1AJwaCT","colab_type":"code","colab":{}},"source":["# Function to show all images\n","def show_images(images, cols = 1, titles = None):\n","    \"\"\"Display a list of images in a single figure with matplotlib.\n","    \n","    Parameters\n","    ---------\n","    images: List of np.arrays compatible with plt.imshow.\n","    \n","    cols (Default = 1): Number of columns in figure (number of rows is \n","                        set to np.ceil(n_images/float(cols))).\n","    \n","    titles: List of titles corresponding to each image. Must have\n","            the same length as titles.\n","    \"\"\"\n","    assert((titles is None)or (len(images) == len(titles)))\n","    n_images = len(images)\n","    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n","    fig = plt.figure()\n","    for n, (image, title) in enumerate(zip(images, titles)):\n","        a = fig.add_subplot(cols, np.ceil(n_images/float(cols)), n + 1)\n","        if image.ndim == 2:\n","            plt.gray()\n","        plt.imshow(image)\n","        plt.axis(\"off\")\n","        a.set_title(title)\n","    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"48h7T9WkuMN0","colab_type":"code","colab":{}},"source":["def image_transformation_hsv(image_name):\n","  image1 = cv.imread(image_name)\n","  image1 = cv.cvtColor(image1, cv.COLOR_BGR2RGB)\n","  img1_hsv = cv.cvtColor(image1, cv.COLOR_RGB2HSV)\n","\n","  # Define the colors.\n","  lower_color = np.array([100,100,100])\n","  upper_color = np.array([179,255,255])\n","\n","  mask = cv.inRange(img1_hsv, lower_color, upper_color)\n","\n","  edged = cv.Canny(mask, 10, 250)\n","  #applying closing function \n","  kernel = cv.getStructuringElement(cv.MORPH_RECT, (7, 7))\n","  closed = cv.morphologyEx(edged, cv.MORPH_CLOSE, kernel)\n","\n","  images = [image1, img1_hsv, mask, edged, kernel, closed]\n","  titles = [\"image\",\"img_hsv\", \"mask\", \"edged\", \"kernel\", \"closed\"]\n","  # show_images(images,1,titles)\n","  return images"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kV2eAhWmdsM3","colab_type":"code","colab":{}},"source":["def image_transformation_hls(image_name):\n","  image2 = cv.imread(image_name)\n","  image2 = cv.cvtColor(image2, cv.COLOR_BGR2RGB)\n","  img2_hls = cv.cvtColor(image2, cv.COLOR_RGB2HLS)\n","  maskes = []\n","  mask = np.zeros((img2_hls.shape[0], img2_hls.shape[1], 1), dtype = \"uint8\")\n","  # Define the colors.\n","  hls_range = [[np.array([172,40,50]), np.array([180,130,130])],\n","               [np.array([0,40,50]), np.array([5,130,130])],\n","               [np.array([145,90,90]), np.array([155,130,130])]]\n","\n","  for hls_ran in hls_range:\n","    maskes.append(cv.inRange(img2_hls, hls_ran[0], hls_ran[1]))\n","  \n","  for mask_solo in maskes:\n","    mask = cv.bitwise_or(mask, mask_solo)\n","\n","  edged = cv.Canny(mask, 10, 250)\n","  #applying closing function \n","  kernel = cv.getStructuringElement(cv.MORPH_RECT, (7, 7))\n","  closed = cv.morphologyEx(edged, cv.MORPH_CLOSE, kernel)\n","\n","  images = [image2, img2_hls, mask, edged, kernel, closed]\n","  titles = [\"image\",\"img2_hls\", \"mask\", \"edged\", \"kernel\", \"closed\"]\n","  # show_images(images,1,titles)\n","  return images"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b5ZOXmG6lPD6","colab_type":"code","colab":{}},"source":["start_w = 0\n","start_h = 0\n","end_w = 0\n","end_h = 0\n","def get_range_contours(x, y, w, h, image_h, image_w):\n","  mid_w = int(x + w/2)\n","  mid_h = int(y + h/2)\n","  if w < h:\n","    h = int((h + (h * 0.5))/2)\n","    w = h\n","  elif w > h:\n","    w = int((w + (w * 0.5))/2)\n","    h = w\n","  start_h = np.maximum(mid_h - h, 0)\n","  end_h = np.minimum(mid_h + h, image_h-1)\n","  start_w = np.maximum(mid_w - w, 0)\n","  end_w = np.minimum(mid_w + w, image_w-1)\n","  return start_h,end_h, start_w, end_w"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YTQJ_4vyz3Ql","colab_type":"code","colab":{}},"source":["def get_contours(image,mask):\n","  (cnts, _) = cv.findContours(mask.copy(), cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n","\n","  idx = 0\n","  contour_images = []\n","  contour_titles = []\n","  bimage = image.copy()\n","  image_size = image.shape[0] * image.shape[1] * 0.001\n","  result_sign = 0\n","  for c in cnts:\n","    area = cv.contourArea(c) \n","    # Shortlisting the regions based on there area. \n","    if area > image_size:\n","      x,y,w,h = cv.boundingRect(c)\n","      if w>50 and h>50:\n","        idx+=1\n","        (sh,eh,sw,ew) = get_range_contours(x,y,w,h,image.shape[0],image.shape[1])\n","        new_img = image[sh:eh,sw:ew]\n","        cv.imwrite(str(idx) + '.png', new_img)\n","        sign_image = cv.resize(new_img, (80, 80))\n","        sign_image = (sign_image.astype('float32') / 255).reshape(-1, 80 * 80 * 3)\n","        contour_images.append(new_img)\n","        pred = model.predict([sign_image])\n","        label = signs[np.argmax(pred)]\n","        result_sign = check_result(np.argmax(pred), result_sign)\n","        contour_titles.append(label)\n","        # print(f\"label: {label} {pred}\")\n","  \n","        cv.putText(bimage, label, (x, y), cv.FONT_HERSHEY_COMPLEX, 1.5, (255, 0, 0), 2 , cv.LINE_AA)\n","        bimage = cv.rectangle(bimage, (sw, sh), (ew, eh), (255,0,0), 2)\n","\n","  show_images(contour_images,1,contour_titles) \n","  show_result_sign(result_sign)\n","  #plt.figure(figsize=(20,20))\n","  #plt.imshow(bimage)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5e3u7r9hWWXa","colab_type":"text"},"source":["# 4. Classification by Contours"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"h9JwskgFWj4e"},"source":["# 5. Machine Learning"]},{"cell_type":"markdown","metadata":{"id":"dyYxgiEpWqnF","colab_type":"text"},"source":["## The dataset\n","\n","The dataset we are using is a modified version of the German Traffic Sign Recognition Benchmark (GTSRB) dataset [1]. \n","\n","[1] http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"b41zTmjoXMFk","scrolled":false,"colab":{}},"source":["!wget -c https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Training_Images.zip -O training_data.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5pvKUd7jT8Gr","colab":{}},"source":["!rm -r GTSRB\n","!rm -r training_data\n","\n","with zipfile.ZipFile(\"training_data.zip\", \"r\") as zip_ref:\n","  zip_ref.extractall()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p29O8MUC-tSa","colab_type":"text"},"source":["We remove all the images that are not relevant to bike traffic (e.g. highway speed limits) and add reference images for each sign type to the dataset."]},{"cell_type":"code","metadata":{"id":"a3bic3pe-KMe","colab_type":"code","colab":{}},"source":["!mv GTSRB/Final_Training/Images training_data/\n","!rm -r training_data/00000\n","!rm -r training_data/00001\n","!rm -r training_data/00002\n","!rm -r training_data/00003\n","!rm -r training_data/00004\n","!rm -r training_data/00005\n","!rm -r training_data/00006\n","!rm -r training_data/00007\n","!rm -r training_data/00008\n","!rm -r training_data/00009\n","!rm -r training_data/00010\n","!mv training_data/00011 training_data/priority\n","!mv training_data/00012 training_data/priority_road\n","!mv training_data/00013 training_data/yield\n","!mv training_data/00014 training_data/stop\n","!mv training_data/00015 training_data/no_entrance\n","!rm -r training_data/00016\n","!mv training_data/00017 training_data/no_entry_vehicles\n","!mv training_data/00018 training_data/danger_ahead\n","!rm -r training_data/00019\n","!rm -r training_data/00020\n","!mv training_data/00021 training_data/double_bend\n","!rm -r training_data/00022\n","!rm -r training_data/00023\n","!rm -r training_data/00024\n","!mv training_data/00025 training_data/road_works\n","!mv training_data/00026 training_data/traffic_signals\n","!mv training_data/00027 training_data/pedestrians_in_road_ahead\n","!mv training_data/00028 training_data/children_crossing_ahead\n","!rm -r training_data/00029\n","!rm -r training_data/00030\n","!rm -r training_data/00031\n","!rm -r training_data/00032\n","!mv training_data/00033 training_data/turn_right_ahead\n","!mv training_data/00034 training_data/turn_left_ahead\n","!mv training_data/00035 training_data/proceed_ahead\n","!mv training_data/00036 training_data/ahead_or_right_only\n","!mv training_data/00037 training_data/ahead_or_left_only\n","!mv training_data/00038 training_data/keep_right\n","!mv training_data/00039 training_data/keep_left\n","!rm -r training_data/00040\n","!rm -r training_data/00041\n","!rm -r training_data/00042\n","!rm $(find . -iname '*.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tw5hf1JOHIDX","colab_type":"code","colab":{}},"source":["!wget -q https://www.dvr.de/bilder/stvo/gt/301.png -O training_data/priority/reference.png\n","!wget -q https://www.dvr.de/bilder/stvo/gt/306.png -O training_data/priority_road/reference.png\n","!wget -q https://www.dvr.de/bilder/stvo/gt/205.png -O training_data/yield/reference.png\n","!wget -q https://www.dvr.de/bilder/stvo/gt/206.png -O training_data/stop/reference.png\n","!wget -q https://www.dvr.de/bilder/stvo/gt/250.png -O training_data/no_entrance/reference.png\n","!wget -q https://www.dvr.de/bilder/stvo/gt/267.png -O training_data/no_entry_vehicles/reference.png\n","!wget -q https://www.dvr.de/bilder/stvo/gt/101.png -O training_data/danger_ahead/reference.png\n","!wget -q https://www.dvr.de/bilder/stvo/gt/105-10.png -O training_data/double_bend/reference.png\n","!wget -q https://www.dvr.de/bilder/stvo/gt/123.png -O training_data/road_works/reference.png\n","!wget -q https://www.dvr.de/bilder/stvo/gt/131.png -O training_data/traffic_signals/reference.png\n","!wget -q https://www.dvr.de/bilder/stvo/gt/133-10.png -O training_data/pedestrians_in_road_ahead/reference.png\n","!wget -q https://www.dvr.de/bilder/stvo/gt/136-10.png -O training_data/children_crossing_ahead/reference.png\n","!wget -q https://www.dvr.de/bilder/stvo/gt/209.png -O training_data/turn_right_ahead/reference.png\n","!wget -q https://www.dvr.de/bilder/stvo/gt/214.png -O training_data/ahead_or_right_only/reference.png\n","!wget -q https://www.dvr.de/bilder/stvo/gt/222.png -O training_data/keep_right/reference.png"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R72X5AWJFtQt","colab_type":"code","colab":{}},"source":["cv.imwrite(\"training_data/turn_left_ahead/reference.png\", cv.flip(cv.imread(\"training_data/turn_right_ahead/reference.png\"), 1))\n","cv.imwrite(\"training_data/keep_left/reference.png\", cv.flip(cv.imread(\"training_data/keep_right/reference.png\"), 1))\n","cv.imwrite(\"training_data/ahead_or_left_only/reference.png\", cv.flip(cv.imread(\"training_data/ahead_or_right_only/reference.png\"), 1))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"r_edL7-TT8Gx"},"source":["We can take a look at the extracted training data."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BmG-92UcT8Gy","colab":{}},"source":["!apt install -qq tree > /dev/null\n","!tree --filelimit 20 training_data/"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0vYzgXe9T8G3"},"source":["It consists of 19 categories of traffic signs that have been extracted from the GTSRB dataset. Each image shows exactly one traffic sign, usually roughly at the center of the image. Some of the traffic signs might be partially ocluded, rotated, or facing away from the camera."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"luCxSgs6T8G3","scrolled":false,"colab":{}},"source":["%matplotlib inline\n","\n","fig = plt.figure(figsize=(20,35))\n","for sign_num, sign_type in enumerate(os.listdir(\"training_data/\")):\n","    sign_path = os.path.join(\"training_data\", sign_type)\n","    images = np.array(os.listdir(sign_path))\n","    \n","    selector = np.random.randint(0, len(sign_path), size=3)\n","    \n","    for j, img in enumerate(images[selector]):\n","        plt.subplot(20, 3, (sign_num * 3) + j + 1)\n","        plt.xlabel(sign_type)\n","        plt.imshow(cv.cvtColor(cv.imread(os.path.join(sign_path, img)), cv.COLOR_BGR2RGB))\n","        \n","fig.tight_layout()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UQk_MRj9T8HB"},"source":["## Data Augmentation"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Or7EHv_ZT8HC"},"source":["Preprocessing the data by augmenting."]},{"cell_type":"code","metadata":{"id":"2pmaZwJNHxjV","colab_type":"code","colab":{}},"source":["for sign_num, sign_type in enumerate(os.listdir(\"training_data/\")):\n","    sign_path = os.path.join(\"training_data\", sign_type)\n","    images = os.listdir(sign_path)\n","    \n","    for image in images:\n","      try:\n","        img = cv.imread(os.path.join(sign_path, image))\n","\n","        if img.shape[0] < 50 or img.shape[1] < 50:\n","            os.remove(os.path.join(sign_path, image))\n","        \n","        elif image.endswith(\"ppm\"):\n","            cv.imwrite(os.path.join(sign_path, image.replace(\"ppm\", \"png\")), img)\n","            os.remove(os.path.join(sign_path, image))\n","      except:\n","        pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OfsBaICiWj5H","colab":{}},"source":["TARGET_NUMBER_OF_IMGS = 2000\n","\n","for sign_num, sign_type in enumerate(os.listdir(\"training_data/\")):\n","    sign_path = os.path.join(\"training_data\", sign_type)\n","    num_images = len(os.listdir(sign_path))\n","\n","    p = Augmentor.Pipeline(sign_path)\n","    print(\"\\n\")\n","\n","    p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n","    p.zoom(probability=0.5, min_factor=1.1, max_factor=1.5)\n","    p.skew(probability=0.3)\n","    #p.gaussian_distortion(probability=0.4, grid_width=10, grid_height=10, magnitude=4, corner=\"bell\", method=\"in\")\n","\n","    if num_images <= TARGET_NUMBER_OF_IMGS:\n","      p.sample(TARGET_NUMBER_OF_IMGS - num_images)\n","    \n","    print(\"\\n\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ABGgsWqWswe8","colab_type":"text"},"source":["We can now move the generated images into the correct directories."]},{"cell_type":"code","metadata":{"id":"H2nheQRpsTsf","colab_type":"code","colab":{}},"source":["for sign_num, sign_type in enumerate(os.listdir(\"training_data/\")):\n","    sign_path = os.path.join(\"training_data\", sign_type)\n","    output_path = os.path.join(sign_path, \"output\")\n","\n","    for filename in os.listdir(output_path):\n","      os.rename(os.path.join(output_path, filename), os.path.join(sign_path, filename))\n","\n","    os.rmdir(os.path.join(sign_path, \"output\"))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"plEtlzgtT8HF"},"source":["The data is processed by loading all images, resizing them to a dimension of 80 by 80 pixels and then storing the images in a NumPy array."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3ii9YXklT8HH","colab":{}},"source":["def process_image(fileName):\n","    img = cv.imread(fileName)\n","    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n","    img = cv.resize(img, (80,80))\n","    return img\n","\n","DATA_PATH = \"training_data/\"\n","\n","X = []\n","y = []\n","\n","num_samples = 0\n","\n","signs = os.listdir(DATA_PATH)\n","\n","for sign in signs:\n","    current_samples = num_samples\n","    \n","    sign_path = os.path.join(DATA_PATH, sign)\n","    \n","    for file in os.listdir(sign_path):\n","        if not (file.endswith(\".ppm\") or file.endswith(\".png\")):\n","            continue\n","        \n","        img = process_image(os.path.join(sign_path, file))\n","        \n","        X.append(img)\n","        y.append(signs.index(sign))\n","        num_samples += 1\n","\n","    print(f\"Read {num_samples - current_samples} samples for {sign}.\")\n","\n","X = np.array(X)\n","y = np.array(y)\n","\n","print(f\"Number of Samples: {num_samples}\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QX5J2sJET8HJ"},"source":["We split the dataset into a training and test set using a 75/25 split (the default setting in train_test_split)."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FykfnV8VWj4y","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7QYln8dpi-rc","colab_type":"text"},"source":["This yields the following distribution for the training and test sets:"]},{"cell_type":"code","metadata":{"id":"0MikDT-yjEZS","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(40,10))\n","plt.bar(signs, [len(y_train[y_train == i]) for i in range(0, len(signs))])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"u7MaCimzT8HM"},"source":["## Extracting image features and learning using a support vector machine"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NcFcuTpjT8HM"},"source":["Our first approach to classifiying the traffic sign images, is using a support vector machine [0] classifier. In order to do that, we first compute a feature vector for each image by computing Histogram of Oriented Gradients (HOG) [1] features for each image.\n","\n","[0] James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani, eds. An Introduction to Statistical Learning: With Applications in R. Springer Texts in Statistics 103. New York: Springer, 2013.\n","\n","[1] https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yXG7pFmGT8HN","colab":{}},"source":["import skimage.feature\n","\n","X_train_hog = np.array([skimage.feature.hog(img) for img in X_train])\n","X_test_hog = np.array([skimage.feature.hog(img) for img in X_test])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LBfWQ8hjT8HP"},"source":["After the conversion the training set now consists of *n* feature vectors, each with a size of 5184 predictors:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GQNCAFGAT8HP","colab":{}},"source":["X_train_hog.shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pZlU_oxNT8HR"},"source":["We now have converted from the original high-dimensional space of dimension `19200 (= 80 * 80 * 3)` to a 5184-dimensional space, which is much more anemable to analysis by a statistical learning algorithm.\n","\n","We can now use the `SGDClassifier` provided by scikit-learn, which implements a support vector machine trained using stochastic gradient descent (SGD)."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kxSLma7bWj4e","colab":{}},"source":["from sklearn import linear_model\n","\n","clf = linear_model.SGDClassifier(max_iter=100, tol=1e-3, n_jobs=-1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4Jr3KTVBT8HS"},"source":["We can now fit the classifier to the training set of HOG feature vectors."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4rCOnZGTT8HT","colab":{}},"source":["clf.fit(X_train_hog, y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HUYPvWRNT8HV"},"source":["We can now compute the training and test accuracy of our model:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"U_8v4DOnT8HW","colab":{}},"source":["print(f\"Training accuracy: {clf.score(X_train_hog, y_train)}\")\n","print(f\"Test accuracy: {clf.score(X_test_hog, y_test)}\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VjlLd7NYWj5G"},"source":["## Deep Learning"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"gjv3kEgXWj5J","colab":{}},"source":["import tensorflow\n","from tensorflow.keras import models, layers, optimizers\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras.utils import to_categorical\n","tensorflow.enable_eager_execution()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xYvCDIBFWj5K","colab":{}},"source":["model = models.Sequential()\n","\n","model.add(layers.Reshape((80, 80, 3), input_shape=(80 * 80 * 3,)))\n","model.add(layers.Conv2D(32, (3, 3), input_shape=(80, 80, 3)))\n","model.add(layers.MaxPool2D((2, 2)))\n","\n","model.add(layers.Conv2D(64, (3, 3)))\n","model.add(layers.MaxPool2D((2, 2)))\n","\n","model.add(layers.Conv2D(128, (3, 3)))\n","model.add(layers.MaxPool2D((2, 2)))\n","\n","model.add(layers.Flatten())\n","\n","model.add(layers.Dense(512, activation='relu'))\n","model.add(layers.Dense(19, activation='softmax'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Na73widKT8Hc","colab":{}},"source":["model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZvR7bRivdHpw","colab":{}},"source":["model.compile(loss='categorical_crossentropy', \n","              optimizer='rmsprop',\n","              metrics=['acc'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fpWryJyWdOzm","colab":{}},"source":["X_train_reshaped = (X_train.astype('float32') / 255.0).reshape(-1, 80 * 80 * 3)\n","\n","model.fit(X_train_reshaped, to_categorical(y_train), validation_split=0.1, epochs=30)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kazPvxKGeCnY","colab":{}},"source":["model.evaluate((X_test.astype('float32') / 255.0).reshape(-1, 80 * 80 * 3), to_categorical(y_test))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"05g4OJQr9__3","colab_type":"code","colab":{}},"source":["from sklearn.metrics import confusion_matrix\n","\n","cm = confusion_matrix(y_test, [np.argmax(p) for p in model.predict((X_test.astype('float32') / 255.0).reshape(-1, 80 * 80 * 3))])\n","\n","print(np.array_str(cm, max_line_width=100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7NK4S7e2v7FN","colab_type":"text"},"source":["## Transfer Learning"]},{"cell_type":"markdown","metadata":{"id":"hvtfIqTsv7FN","colab_type":"text"},"source":["We are using the VGG16 model, provided by keras and trained on the ??? dataset."]},{"cell_type":"code","metadata":{"id":"1GrBq61lv7FO","colab_type":"code","colab":{}},"source":["conv_base = VGG16(weights='imagenet',\n","                  include_top=False,\n","                  input_shape=(80, 80, 3))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WlaSUtogv7FP","colab_type":"text"},"source":["The network has the following architecture:"]},{"cell_type":"code","metadata":{"id":"rwFcqmwOv7FP","colab_type":"code","colab":{}},"source":["conv_base.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A9_YyQ28v7FQ","colab_type":"code","colab":{}},"source":["conv_base.trainable = True\n","\n","for layer in conv_base.layers:\n","  layer.trainable = (layer.name == 'block5_conv1')\n","\n","transfer_model = models.Sequential()\n","\n","transfer_model.add(conv_base)\n","\n","transfer_model.add(layers.Flatten())\n","transfer_model.add(layers.Dense(512, activation='relu'))\n","transfer_model.add(layers.Dense(19, activation='softmax'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zV92zW3wv7FR","colab_type":"code","colab":{}},"source":["transfer_model.compile(loss='categorical_crossentropy', \n","                       optimizer='rmsprop',\n","                       metrics=['acc'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PpHB81vpv7FS","colab_type":"code","colab":{}},"source":["transfer_model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wA75027Hv7FT","colab_type":"code","colab":{}},"source":["X_train_reshaped = (X_train.astype('float32') / 255.0)\n","\n","transfer_model.fit(X_train_reshaped, to_categorical(y_train), epochs=10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F3b0HogiLu0d","colab_type":"text"},"source":["## Import and analyse a Picture"]},{"cell_type":"code","metadata":{"id":"mw79YOdkL-ZA","colab_type":"code","colab":{}},"source":["from google.colab import files\n","uploaded = files.upload()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"48eFxovgfFVW","colab_type":"code","colab":{}},"source":["for fn in uploaded.keys():\n","  load_image(fn)\n","  image_return = image_transformation_hsv(fn)\n","  get_contours(image_return[0], image_return[2])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lUQrh-A4hN3Y","colab_type":"code","colab":{}},"source":["for fn in uploaded.keys():\n","  load_image(fn)\n","  image_return = image_transformation_hls(fn)\n","  get_contours(image_return[0], image_return[2])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gzKnx4bzW2tQ","colab_type":"text"},"source":["# 6. Conclusion"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"e6czrwtbT8Ho"},"source":["## Running on mobile devices"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"r1dfDq_LdJd3","colab":{}},"source":["import tensorflow\n","transfer_model.save(\"model.savedmodel\", save_format=tensorflow.keras.experimental.export_saved_model)\n","#converter = tensorflow.lite.TFLiteConverter.from_keras_model_file(\"model.h5\")\n","#\n","tflite_model = converter.convert()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fGhDFVFwk7wA","colab":{}},"source":["f = open(\"traffic_sign_model.tflite\", \"w+b\")\n","f.write(tflite_model)\n","f.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"idCOtsmWv7FV","colab_type":"code","colab":{}},"source":["interpreter = tensorflow.lite.Interpreter(model_content=tflite_model)\n","interpreter.allocate_tensors()\n","\n","# Get input and output tensors.\n","input_details = interpreter.get_input_details()\n","output_details = interpreter.get_output_details()\n","\n","# Test the TensorFlow Lite model on random input data.\n","interpreter.set_tensor(input_details[0]['index'], [X_train[1300].astype('float32') / 255.0])\n","\n","interpreter.invoke()\n","\n","# The function `get_tensor()` returns a copy of the tensor data.\n","# Use `tensor()` in order to get a pointer to the tensor.\n","interpreter.get_tensor(output_details[0]['index'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NAwr4eGVZuum","colab_type":"text"},"source":["## Object detection"]},{"cell_type":"markdown","metadata":{"id":"IYme2tH9aJHF","colab_type":"text"},"source":["We implement object detection using a convolutional neural network based on the SSD [0] architecture. Specifically, we implement the SSD300 version (i.e. the input images have a size of 300x300 pixels) and we use the VGG16 network architecture for the image classification part of the SSD model.\n","\n","[0] W. Liu et al., SSD: Single Shot MultiBox Detector, 2016, https://arxiv.org/pdf/1512.02325.pdf"]},{"cell_type":"code","metadata":{"id":"y46smXEq9EGN","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","import tensorflow.keras.backend as K\n","from tensorflow.keras.layers import InputSpec, Layer\n","\n","class AnchorBoxes(Layer):\n","    '''\n","    A Keras layer to create an output tensor containing anchor box coordinates\n","    and variances based on the input tensor and the passed arguments.\n","\n","    A set of 2D anchor boxes of different aspect ratios is created for each spatial unit of\n","    the input tensor. The number of anchor boxes created per unit depends on the arguments\n","    `aspect_ratios` and `two_boxes_for_ar1`, in the default case it is 4. The boxes\n","    are parameterized by the coordinate tuple `(xmin, xmax, ymin, ymax)`.\n","\n","    The logic implemented by this layer is identical to the logic in the module\n","    `ssd_box_encode_decode_utils.py`.\n","\n","    The purpose of having this layer in the network is to make the model self-sufficient\n","    at inference time. Since the model is predicting offsets to the anchor boxes\n","    (rather than predicting absolute box coordinates directly), one needs to know the anchor\n","    box coordinates in order to construct the final prediction boxes from the predicted offsets.\n","    If the model's output tensor did not contain the anchor box coordinates, the necessary\n","    information to convert the predicted offsets back to absolute coordinates would be missing\n","    in the model output. The reason why it is necessary to predict offsets to the anchor boxes\n","    rather than to predict absolute box coordinates directly is explained in `README.md`.\n","\n","    Input shape:\n","        4D tensor of shape `(batch, channels, height, width)` if `dim_ordering = 'th'`\n","        or `(batch, height, width, channels)` if `dim_ordering = 'tf'`.\n","\n","    Output shape:\n","        5D tensor of shape `(batch, height, width, n_boxes, 8)`. The last axis contains\n","        the four anchor box coordinates and the four variance values for each box.\n","    '''\n","\n","    def __init__(self,\n","                 img_height,\n","                 img_width,\n","                 this_scale,\n","                 next_scale,\n","                 aspect_ratios=[0.5, 1.0, 2.0],\n","                 this_steps=None,\n","                 this_offsets=None,\n","                 variances=[0.1, 0.1, 0.2, 0.2],\n","                 **kwargs):\n","      \n","        variances = np.array(variances)\n","        \n","        self.img_height = img_height\n","        self.img_width = img_width\n","        self.this_scale = this_scale\n","        self.next_scale = next_scale\n","        self.aspect_ratios = aspect_ratios\n","        self.this_steps = this_steps\n","        self.this_offsets = this_offsets\n","        self.variances = variances\n","        self.n_boxes = len(aspect_ratios) + 1\n","        \n","        super(AnchorBoxes, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.input_spec = [InputSpec(shape=input_shape)]\n","        super(AnchorBoxes, self).build(input_shape)\n","\n","    def call(self, x, mask=None):\n","        # Compute box width and height for each aspect ratio\n","        # The shorter side of the image will be used to compute `w` and `h` using `scale` and `aspect_ratios`.\n","        size = min(self.img_height, self.img_width)\n","        # Compute the box widths and and heights for all aspect ratios\n","        wh_list = []\n","        for ar in self.aspect_ratios:\n","            if (ar == 1):\n","                # Compute the regular anchor box for aspect ratio 1.\n","                box_height = box_width = self.this_scale * size\n","                wh_list.append((box_width, box_height))\n","                \n","                # Compute one slightly larger version using the geometric mean of this scale value and the next.\n","                box_height = box_width = np.sqrt(self.this_scale * self.next_scale) * size\n","                wh_list.append((box_width, box_height))\n","            else:\n","                box_height = self.this_scale * size / np.sqrt(ar)\n","                box_width = self.this_scale * size * np.sqrt(ar)\n","                wh_list.append((box_width, box_height))\n","        wh_list = np.array(wh_list)\n","\n","        # We need the shape of the input tensor\n","        batch_size, feature_map_height, feature_map_width, feature_map_channels = x.shape\n","\n","        batch_size = batch_size.value\n","        feature_map_height = feature_map_height.value\n","        feature_map_width = feature_map_width.value\n","        feature_map_channels = feature_map_channels.value\n","        \n","        # Compute the grid of box center points. They are identical for all aspect ratios.\n","\n","        # Compute the step sizes, i.e. how far apart the anchor box center points will be vertically and horizontally.\n","        if (self.this_steps is None):\n","            step_height = self.img_height / feature_map_height\n","            step_width = self.img_width / feature_map_width\n","        else:\n","            if isinstance(self.this_steps, (list, tuple)) and (len(self.this_steps) == 2):\n","                step_height = self.this_steps[0]\n","                step_width = self.this_steps[1]\n","            elif isinstance(self.this_steps, (int, float)):\n","                step_height = self.this_steps\n","                step_width = self.this_steps\n","        # Compute the offsets, i.e. at what pixel values the first anchor box center point will be from the top and from the left of the image.\n","        if (self.this_offsets is None):\n","            offset_height = 0.5\n","            offset_width = 0.5\n","        else:\n","            if isinstance(self.this_offsets, (list, tuple)) and (len(self.this_offsets) == 2):\n","                offset_height = self.this_offsets[0]\n","                offset_width = self.this_offsets[1]\n","            elif isinstance(self.this_offsets, (int, float)):\n","                offset_height = self.this_offsets\n","                offset_width = self.this_offsets\n","        # Now that we have the offsets and step sizes, compute the grid of anchor box center points.\n","        cy = np.linspace(offset_height * step_height, (offset_height + feature_map_height - 1) * step_height, feature_map_height)\n","        cx = np.linspace(offset_width * step_width, (offset_width + feature_map_width - 1) * step_width, feature_map_width)\n","        cx_grid, cy_grid = np.meshgrid(cx, cy)\n","        cx_grid = np.expand_dims(cx_grid, -1) # This is necessary for np.tile() to do what we want further down\n","        cy_grid = np.expand_dims(cy_grid, -1) # This is necessary for np.tile() to do what we want further down\n","\n","        # Create a 4D tensor template of shape `(feature_map_height, feature_map_width, n_boxes, 4)`\n","        # where the last dimension will contain `(cx, cy, w, h)`\n","        boxes_tensor = np.zeros((feature_map_height, feature_map_width, self.n_boxes, 4))\n","\n","        boxes_tensor[:, :, :, 0] = np.tile(cx_grid, (1, 1, self.n_boxes)) # Set cx\n","        boxes_tensor[:, :, :, 1] = np.tile(cy_grid, (1, 1, self.n_boxes)) # Set cy\n","        boxes_tensor[:, :, :, 2] = wh_list[:, 0] # Set w\n","        boxes_tensor[:, :, :, 3] = wh_list[:, 1] # Set h\n","\n","        # Create a tensor to contain the variances and append it to `boxes_tensor`. This tensor has the same shape\n","        # as `boxes_tensor` and simply contains the same 4 variance values for every position in the last axis.\n","        variances_tensor = np.zeros_like(boxes_tensor) # Has shape `(feature_map_height, feature_map_width, n_boxes, 4)`\n","        variances_tensor += self.variances # Long live broadcasting\n","        # Now `boxes_tensor` becomes a tensor of shape `(feature_map_height, feature_map_width, n_boxes, 8)`\n","        boxes_tensor = np.concatenate((boxes_tensor, variances_tensor), axis=-1)\n","\n","        # Now prepend one dimension to `boxes_tensor` to account for the batch size and tile it along\n","        # The result will be a 5D tensor of shape `(batch_size, feature_map_height, feature_map_width, n_boxes, 8)`\n","        boxes_tensor = np.expand_dims(boxes_tensor, axis=0)\n","        boxes_tensor = K.tile(K.constant(boxes_tensor, dtype='float32'), (K.shape(x)[0], 1, 1, 1, 1))\n","\n","        return boxes_tensor\n","\n","    def compute_output_shape(self, input_shape):\n","        batch_size, feature_map_height, feature_map_width, feature_map_channels = input_shape\n","        \n","        return (batch_size, feature_map_height, feature_map_width, self.n_boxes, 8)\n","\n","    def get_config(self):\n","        config = {\n","            'img_height': self.img_height,\n","            'img_width': self.img_width,\n","            'this_scale': self.this_scale,\n","            'next_scale': self.next_scale,\n","            'aspect_ratios': list(self.aspect_ratios),\n","            'variances': list(self.variances)\n","        }\n","        base_config = super(AnchorBoxes, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PoRMSuLEZyJG","colab_type":"code","colab":{}},"source":["from tensorflow.keras import layers, initializers, Model\n","\n","class SSDModel:\n","\n","  num_classes = 0\n","\n","  def __init__(self, num_classes, img_height, img_width):\n","    self.num_classes = num_classes\n","\n","    # Aspect ratios per layer (the numbers are taken from the SSD paper)\n","    aspect_ratios_per_layer = [[1.0, 2.0, 0.5],\n","                               [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n","                               [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n","                               [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n","                               [1.0, 2.0, 0.5],\n","                               [1.0, 2.0, 0.5]]\n","\n","    # The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets\n","    scales = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05]\n","\n","    # The space between two adjacent anchor box center points for each predictor layer.\n","    steps = [8, 16, 32, 64, 100, 300]\n","\n","    # The offsets of the first anchor box center points from the top and left borders\n","    offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n","\n","    # The number of anchor boxes per cell for each predictor layer.\n","    num_of_boxes = [len(ar) + 1 for ar in aspect_ratios_per_layer]\n","    \n","    inp = layers.Input(shape=(img_height, img_width, 3))\n","\n","    feature_extractor, conv4_3 = self.__get_feature_extractor(inp)\n","\n","    # SSD extra feature layers begin here\n","\n","    conv6 = layers.Conv2D(1024, (3, 3), dilation_rate=(6, 6), padding='same')(feature_extractor)\n","\n","    conv7 = layers.Conv2D(1024, (1, 1), padding='same')(conv6)\n","\n","    conv8_1 = layers.Conv2D(256, (1, 1), padding='same')(conv7)\n","    conv8_1 = layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name='conv8_padding')(conv8_1)\n","\n","    conv8_2 = layers.Conv2D(512, (3, 3), strides=(2, 2), padding='valid')(conv8_1)\n","\n","    conv9_1 = layers.Conv2D(128, (1, 1), padding='same')(conv8_2)\n","    conv9_1 = layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name='conv9_padding')(conv9_1)\n","\n","    conv9_2 = layers.Conv2D(256, (3, 3), strides=(2, 2), padding='valid')(conv9_1)\n","\n","    conv10_1 = layers.Conv2D(128, (1, 1), padding='same')(conv9_2)\n","    conv10_2 = layers.Conv2D(256, (3, 3), strides=(1, 1), padding='valid')(conv10_1)\n","\n","    conv11_1 = layers.Conv2D(128, (1, 1), padding='same')(conv10_2)\n","    conv11_2 = layers.Conv2D(256, (3, 3), strides=(1, 1), padding='valid')(conv11_1)\n","\n","    # Feed conv4_3 into a normalization layer since the gradient can get\n","    # very large.\n","    conv4_3_norm = layers.LayerNormalization(gamma_initializer=initializers.Constant(20), name='conv4_3_norm')(conv4_3)\n","\n","    # These are the convolutional class predictors, i.e. each of these layers has an\n","    # output shape of (batch, height, width, num_of_boxes * num_classes), which \n","    # means that it outputs a prediction (vector of length num_classes) for each \n","    # box for each position in the feature map.\n","    conv4_3_norm_pred = layers.Conv2D(num_of_boxes[0] * num_classes, (3, 3), padding='same', name='conv4_3_norm_pred')(conv4_3_norm)\n","    conv7_pred = layers.Conv2D(num_of_boxes[1] * num_classes, (3, 3), padding='same', name='conv7_pred')(conv7)\n","    conv8_2_pred = layers.Conv2D(num_of_boxes[2] * num_classes, (3, 3), padding='same', name='conv8_2_pred')(conv8_2)\n","    conv9_2_pred = layers.Conv2D(num_of_boxes[3] * num_classes, (3, 3), padding='same', name='conv9_2_pred')(conv9_2)\n","    conv10_2_pred = layers.Conv2D(num_of_boxes[4] * num_classes, (3, 3), padding='same', name='conv10_2_pred')(conv10_2)\n","    conv11_2_pred = layers.Conv2D(num_of_boxes[5] * num_classes, (3, 3), padding='same', name='conv11_2_pred')(conv11_2)\n","\n","    # These are the convolutional bounding box predictors, i.e. each of these \n","    # layers has an output shape of (batch, height, width, num_of_boxes * 4), which \n","    # means that it outputs a predicted bounding box (xmin, ymin, xmax, ymax)\n","    # for each anchor box.\n","    conv4_3_norm_bbox = layers.Conv2D(num_of_boxes[0] * 4, (3, 3), padding='same', name='conv4_3_norm_bbox')(conv4_3_norm)\n","    conv7_bbox = layers.Conv2D(num_of_boxes[1] * 4, (3, 3), padding='same', name='conv7_bbox')(conv7)\n","    conv8_2_bbox = layers.Conv2D(num_of_boxes[2] * 4, (3, 3), padding='same', name='conv8_2_bbox')(conv8_2)\n","    conv9_2_bbox = layers.Conv2D(num_of_boxes[3] * 4, (3, 3), padding='same', name='conv9_2_bbox')(conv9_2)\n","    conv10_2_bbox = layers.Conv2D(num_of_boxes[4] * 4, (3, 3), padding='same', name='conv10_2_bbox')(conv10_2)\n","    conv11_2_bbox = layers.Conv2D(num_of_boxes[5] * 4, (3, 3), padding='same', name='conv11_2_bbox')(conv11_2)\n","\n","    # Output shape of anchors: `(batch, height, width, n_boxes, 8)`\n","    conv4_3_norm_anchor = AnchorBoxes(img_height, img_width, this_scale=scales[0], next_scale=scales[1], aspect_ratios=aspect_ratios_per_layer[0],\n","                                             this_steps=steps[0], this_offsets=offsets[0], name='conv4_3_norm_anchor')(conv4_3_norm_bbox)\n","\n","    conv7_anchor = AnchorBoxes(img_height, img_width, this_scale=scales[1], next_scale=scales[2], aspect_ratios=aspect_ratios_per_layer[1],\n","                                    this_steps=steps[1], this_offsets=offsets[1], name='conv7_anchor')(conv7_bbox)\n","\n","    conv8_2_anchor = AnchorBoxes(img_height, img_width, this_scale=scales[2], next_scale=scales[3], aspect_ratios=aspect_ratios_per_layer[2],\n","                                        this_steps=steps[2], this_offsets=offsets[2], name='conv8_2_anchor')(conv8_2_bbox)\n","\n","    conv9_2_anchor = AnchorBoxes(img_height, img_width, this_scale=scales[3], next_scale=scales[4], aspect_ratios=aspect_ratios_per_layer[3],\n","                                        this_steps=steps[3], this_offsets=offsets[3], name='conv9_2_anchor')(conv9_2_bbox)\n","\n","    conv10_2_anchor = AnchorBoxes(img_height, img_width, this_scale=scales[4], next_scale=scales[5], aspect_ratios=aspect_ratios_per_layer[4],\n","                                        this_steps=steps[4], this_offsets=offsets[4], name='conv10_2_anchor')(conv10_2_bbox)\n","\n","    conv11_2_anchor = AnchorBoxes(img_height, img_width, this_scale=scales[5], next_scale=scales[6], aspect_ratios=aspect_ratios_per_layer[5],\n","                                        this_steps=steps[5], name='conv11_2_anchor')(conv11_2_bbox)\n","\n","    ### Reshape\n","\n","    # Reshape the class predictions, yielding 3D tensors of shape `(batch, height * width * n_boxes, n_classes)`\n","    # We want the classes isolated in the last axis to perform softmax on them\n","    conv4_3_norm_pred_reshape = layers.Reshape((-1, num_classes), name='conv4_3_norm_pred_reshape')(conv4_3_norm_pred)\n","    conv7_pred_reshape = layers.Reshape((-1, num_classes), name='conv7_pred_reshape')(conv7_pred)\n","    conv8_2_pred_reshape = layers.Reshape((-1, num_classes), name='conv8_2_pred_reshape')(conv8_2_pred)\n","    conv9_2_pred_reshape = layers.Reshape((-1, num_classes), name='conv9_2_pred_reshape')(conv9_2_pred)\n","    conv10_2_pred_reshape = layers.Reshape((-1, num_classes), name='conv10_2_pred_reshape')(conv10_2_pred)\n","    conv11_2_pred_reshape = layers.Reshape((-1, num_classes), name='conv11_2_pred_reshape')(conv11_2_pred)\n","\n","    # Reshape the box predictions, yielding 3D tensors of shape `(batch, height * width * n_boxes, 4)`\n","    # We want the four box coordinates isolated in the last axis to compute the smooth L1 loss\n","    conv4_3_norm_bbox_reshape = layers.Reshape((-1, 4), name='conv4_3_norm_bbox_reshape')(conv4_3_norm_bbox)\n","    conv7_bbox_reshape = layers.Reshape((-1, 4), name='conv7_bbox_reshape')(conv7_bbox)\n","    conv8_2_bbox_reshape = layers.Reshape((-1, 4), name='conv8_2_bbox_reshape')(conv8_2_bbox)\n","    conv9_2_bbox_reshape = layers.Reshape((-1, 4), name='conv9_2_bbox_reshape')(conv9_2_bbox)\n","    conv10_2_bbox_reshape = layers.Reshape((-1, 4), name='conv10_2_bbox_reshape')(conv10_2_bbox)\n","    conv11_2_bbox_reshape = layers.Reshape((-1, 4), name='conv11_2_bbox_reshape')(conv11_2_bbox)\n","\n","    # Reshape the anchor box tensors, yielding 3D tensors of shape `(batch, height * width * n_boxes, 8)`\n","    conv4_3_norm_anchor_reshape = layers.Reshape((-1, 8), name='conv4_3_norm_anchor_reshape')(conv4_3_norm_anchor)\n","    conv7_anchor_reshape = layers.Reshape((-1, 8), name='fc7_anchor_reshape')(conv7_anchor)\n","    conv8_2_anchor_reshape = layers.Reshape((-1, 8), name='conv6_2_anchor_reshape')(conv8_2_anchor)\n","    conv9_2_anchor_reshape = layers.Reshape((-1, 8), name='conv7_2_anchor_reshape')(conv9_2_anchor)\n","    conv10_2_anchor_reshape = layers.Reshape((-1, 8), name='conv8_2_anchor_reshape')(conv10_2_anchor)\n","    conv11_2_anchor_reshape = layers.Reshape((-1, 8), name='conv9_2_anchor_reshape')(conv11_2_anchor)\n","\n","    ### Concatenate the predictions from the different layers\n","\n","    # Axis 0 (batch) and axis 2 (n_classes or 4, respectively) are identical for all layer predictions,\n","    # so we want to concatenate along axis 1, the number of boxes per layer\n","    # Output shape of `mbox_conf`: (batch, n_boxes_total, n_classes)\n","    mbox_conf = layers.Concatenate(axis=1, name='mbox_conf')([conv4_3_norm_pred_reshape,\n","                                                       conv7_pred_reshape,\n","                                                       conv8_2_pred_reshape,\n","                                                       conv9_2_pred_reshape,\n","                                                       conv10_2_pred_reshape,\n","                                                       conv11_2_pred_reshape])\n","\n","    # Output shape of `mbox_loc`: (batch, n_boxes_total, 4)\n","    mbox_loc = layers.Concatenate(axis=1, name='mbox_loc')([conv4_3_norm_bbox_reshape,\n","                                                     conv7_bbox_reshape,\n","                                                     conv8_2_bbox_reshape,\n","                                                     conv9_2_bbox_reshape,\n","                                                     conv10_2_bbox_reshape,\n","                                                     conv11_2_bbox_reshape])\n","\n","    # Output shape of `mbox_priorbox`: (batch, n_boxes_total, 8)\n","    mbox_priorbox = layers.Concatenate(axis=1, name='mbox_priorbox')([conv4_3_norm_anchor_reshape,\n","                                                               conv7_anchor_reshape,\n","                                                               conv8_2_anchor_reshape,\n","                                                               conv9_2_anchor_reshape,\n","                                                               conv10_2_anchor_reshape,\n","                                                               conv11_2_anchor_reshape])\n","\n","    # The box coordinate predictions will go into the loss function just the way they are,\n","    # but for the class predictions, we'll apply a softmax activation layer first\n","    mbox_conf_softmax = layers.Activation('softmax', name='mbox_conf_softmax')(mbox_conf)\n","\n","    # Concatenate the class and box predictions and the anchors to one large predictions vector\n","    # Output shape of `predictions`: (batch, n_boxes_total, n_classes + 4 + 8)\n","    predictions = layers.Concatenate(axis=2, name='predictions')([mbox_conf_softmax, mbox_loc, mbox_priorbox])\n","\n","    self.model = Model(inputs=inp, outputs=predictions)\n","\n","  \"\"\"\n","  Constructs the feature extractor part of the SSD network. Default is VGG16.\n","  \"\"\"\n","  def __get_feature_extractor(self, input_layer):\n","    # Block 1\n","    x = layers.Conv2D(64, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block1_conv1')(input_layer)\n","    x = layers.Conv2D(64, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block1_conv2')(x)\n","    x = layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='block1_pool')(x)\n","\n","    # Block 2\n","    x = layers.Conv2D(128, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block2_conv1')(x)\n","    x = layers.Conv2D(128, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block2_conv2')(x)\n","    x = layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='block2_pool')(x)\n","\n","    # Block 3\n","    x = layers.Conv2D(256, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block3_conv1')(x)\n","    x = layers.Conv2D(256, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block3_conv2')(x)\n","    x = layers.Conv2D(256, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block3_conv3')(x)\n","    x = layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='block3_pool')(x)\n","\n","    # Block 4\n","    x = layers.Conv2D(512, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block4_conv1')(x)\n","    x = layers.Conv2D(512, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block4_conv2')(x)\n","    x = layers.Conv2D(512, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block4_conv3')(x)\n","    conv4_3 = x\n","    x = layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='block4_pool')(x)\n","\n","    # Block 5\n","    x = layers.Conv2D(512, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block5_conv1')(x)\n","    x = layers.Conv2D(512, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block5_conv2')(x)\n","    x = layers.Conv2D(512, (3, 3),\n","                      activation='relu',\n","                      padding='same',\n","                      name='block5_conv3')(x)\n","    x = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same', name='block5_pool')(x)\n","\n","    return x, conv4_3\n","\n","\n","ssd = SSDModel(19, 300, 300)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6kgrxoBRuoGg","colab_type":"code","colab":{}},"source":["ssd.model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HpOIX_CMfPAC","colab_type":"code","colab":{}},"source":["from IPython.display import SVG\n","from tensorflow.keras.utils import model_to_dot\n","\n","SVG(model_to_dot(ssd.model).create(prog='dot', format='svg'))"],"execution_count":0,"outputs":[]}]}